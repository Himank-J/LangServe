# LangServe

## What is LangServe?

- LangServe, a key component of the LangChain framework, simplifies the development of interfaces for Generative AI applications. It helps in the deployment of LangChain runnables and chains as REST APIs, serving as a bridge between the powerful LLMs and user-friendly interactions. By eliminating complexity, LangServe enables developers to create APIs effortlessly, providing seamless communication between users and AI systems.
- Integrated with FastAPI and leveraging pydantic for data validation, LangServe accelerates the development of innovative AI solutions. Its streamlined approach empowers developers to focus on the core functionalities of their applications, allowing them to experiment and iterate more efficiently. This helps to add more innovation to the Generative AI applications.

## Why it exists

Whether youâ€™re building a customer-facing chatbot, or an internal tool powered by LLMs, youâ€™ll probably start by building a prototype (maybe in a Jupyter notebook), and iterate on it until itâ€™s good enough to get people using it.

And thatâ€™s where LangServe comes in, LangChain has taken their experience of scaling applications in production, and made it available as a python package you can use for your own LLM apps.

ðŸ’¡
Your Idea + LCEL = **Prototype** + LangServe = **Production-ready API** + [Hosting Provider] = **Live deployment** + LangSmith Tracing = **Monitor your production deployment**

## File Structure

- langserve_api_basic - Simplest way of integrating langserve with your LLM models. With few lines you can create your API endpoint using the true power of Langserve.
  - Endpoint - localhost:8000/<path>/invoke (read more in langserve documentation)
  - Input: POST request JSON <br>
  
    ~~~
    {
      "input" : {
        "<var_name>": "<text>"
      }
    }
    ~~~

   Replace with your variable name as declared in prompt template
      
- Custom Input - This directory displays how to use langserve's custom data type to define input and output schema. This is very similar to pydantic and acts as a data validation
  - Endpoint - localhost:8000/<path>/invoke (read more in langserve documentation)
  - Input: POST request JSON <br>
  
    ~~~
    {
      "input" : {
        "<var_name>": "<text>"
      }
    }
    ~~~
    Replace with your variable name as declared in input schema template (custom data type)

## LangServe Playground

When you use LangServe to deploy your chain you get for free a playground experience. 

Why is this useful?

**First**, this immediately provides a (simple) UI for your chains and agents. Although simple, this UI does have necessary things like:

- Streaming outputs
- Full log of intermediate steps
- Configurable options

This will make it possible to share a link with colleagues and let them interact with in the UI, facilitating collaboration among a larger team. Specifically, we imagine this being a way for engineers to easily expose a way for non-technical folks to interact with their chains/agents (without having to connect it to the frontend).

**Second**, this provides a way to experiment with different parameters. In this playground you can change the values of certain, configurable parameters (more on that later) as well as try out different inputs and get the response streamed back in real time.

Below are few screenshots of playground generated by LangServe

### Sample 1 - Basic API

<img width="437" alt="Screenshot 2024-05-05 at 7 19 40â€¯PM" src="https://github.com/Himank-J/LangServe/assets/55919214/cafa6899-729a-451b-8793-9f3ced7fc486">

### Sample 2 - Custom Input API

<img width="433" alt="Screenshot 2024-05-05 at 7 16 12â€¯PM" src="https://github.com/Himank-J/LangServe/assets/55919214/9e67b825-7005-4e94-921b-1b6a961351f6">




